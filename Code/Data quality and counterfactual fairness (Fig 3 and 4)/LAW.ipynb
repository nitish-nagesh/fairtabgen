{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f4fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                Precision↑      Recall↑         AUROC↑          FTU↓            DP↓            \n",
      "-----------------------------------------------------------------------------------------------\n",
      "Decision Tree        0.958 ± 0.002   0.946 ± 0.003   0.583 ± 0.007   0.083 ± 0.023   0.132 ± 0.009  \n",
      "Logistic Regression  0.953 ± 0.003   0.996 ± 0.000   0.855 ± 0.005   0.008 ± 0.002   0.049 ± 0.004  \n",
      "Random Forest        0.955 ± 0.003   0.991 ± 0.002   0.807 ± 0.008   0.010 ± 0.002   0.067 ± 0.008  \n",
      "SVM                  0.949 ± 0.003   1.000 ± 0.000   0.565 ± 0.055   0.000 ± 0.000   0.000 ± 0.000  \n",
      "XGBoost              0.955 ± 0.003   0.990 ± 0.001   0.826 ± 0.006   0.010 ± 0.001   0.074 ± 0.007  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# === Load dataset ===\n",
    "df = pd.read_csv(\"bar_pass_prediction (processed version).csv\")    #Original dataset\n",
    "df[\"bar_passed\"] = df[\"bar_passed\"].astype(int)\n",
    "sensitive_col = 'race'  # Change if needed\n",
    "\n",
    "# === Prepare features and labels ===\n",
    "X = df.drop(columns=[\"bar_passed\"])\n",
    "y = df[\"bar_passed\"]\n",
    "\n",
    "# === Initialize models ===\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
    "}\n",
    "\n",
    "# === Repeated Evaluation (for mean ± std) ===\n",
    "N_REPEATS = 5\n",
    "results = {model: {\"Precision\": [], \"Recall\": [], \"AUC\": [], \"FTU\": [], \"DP\": []} for model in models}\n",
    "\n",
    "for seed in range(N_REPEATS):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "        # Save metrics\n",
    "        results[name][\"Precision\"].append(precision_score(y_test, y_pred))\n",
    "        results[name][\"Recall\"].append(recall_score(y_test, y_pred))\n",
    "        results[name][\"AUC\"].append(roc_auc_score(y_test, y_prob) if y_prob is not None else np.nan)\n",
    "\n",
    "        # === FTU ===\n",
    "        X_test_flipped = X_test.copy()\n",
    "        if sensitive_col in X_test_flipped.columns:\n",
    "            X_test_flipped[sensitive_col] = 1 - X_test_flipped[sensitive_col]\n",
    "            y_pred_flipped = model.predict(X_test_flipped)\n",
    "            ftu = np.mean(np.abs(y_pred - y_pred_flipped))\n",
    "        else:\n",
    "            ftu = np.nan\n",
    "        results[name][\"FTU\"].append(ftu)\n",
    "\n",
    "        # === DP (Demographic Parity) ===\n",
    "        if sensitive_col in X_test.columns:\n",
    "            mask_priv = X_test[sensitive_col] == 7\n",
    "            mask_unpriv = ~mask_priv\n",
    "            p_priv = y_pred[mask_priv].mean() if np.any(mask_priv) else 0\n",
    "            p_unpriv = y_pred[mask_unpriv].mean() if np.any(mask_unpriv) else 0\n",
    "            dp = abs(p_priv - p_unpriv)\n",
    "        else:\n",
    "            dp = np.nan\n",
    "        results[name][\"DP\"].append(dp)\n",
    "\n",
    "# === Format Results Table ===\n",
    "def format_metric(values):\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    return f\"{mean:.3f} ± {std:.3f}\"\n",
    "\n",
    "# Print Header\n",
    "print(f\"{'Model':<20} {'Precision↑':<15} {'Recall↑':<15} {'AUROC↑':<15} {'FTU↓':<15} {'DP↓':<15}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "# Print formatted table\n",
    "for model in models:\n",
    "    row = [model]\n",
    "    for metric in [\"Precision\", \"Recall\", \"AUC\", \"FTU\", \"DP\"]:\n",
    "        row.append(format_metric(results[model][metric]))\n",
    "    print(f\"{row[0]:<20} {row[1]:<15} {row[2]:<15} {row[3]:<15} {row[4]:<15} {row[5]:<15}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61abf42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                Precision↑      Recall↑         AUROC↑          FTU↓            DP↓            \n",
      "-----------------------------------------------------------------------------------------------\n",
      "Decision Tree        0.906 ± 0.023   0.926 ± 0.020   0.914 ± 0.012   0.056 ± 0.018   0.051 ± 0.021  \n",
      "Logistic Regression  0.958 ± 0.015   0.937 ± 0.017   0.988 ± 0.004   0.042 ± 0.010   0.059 ± 0.026  \n",
      "Random Forest        0.958 ± 0.020   0.931 ± 0.016   0.985 ± 0.005   0.022 ± 0.007   0.057 ± 0.022  \n",
      "SVM                  0.941 ± 0.022   0.916 ± 0.018   0.982 ± 0.005   0.010 ± 0.004   0.048 ± 0.027  \n",
      "XGBoost              0.942 ± 0.020   0.929 ± 0.020   0.983 ± 0.006   0.032 ± 0.010   0.058 ± 0.026  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# === Load dataset ===\n",
    "df = pd.read_csv(\"generated_data_Our_prompts_Law.csv\")    # Our prompt with Law data\n",
    "df[\"bar_passed\"] = df[\"bar_passed\"].astype(int)\n",
    "sensitive_col = 'race'  # Update this if the sensitive feature is different\n",
    "\n",
    "# === Prepare features and labels ===\n",
    "X = df.drop(columns=[\"bar_passed\"])\n",
    "y = df[\"bar_passed\"]\n",
    "\n",
    "# === Initialize models ===\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
    "}\n",
    "\n",
    "# === Repeated Evaluation (for mean ± std) ===\n",
    "N_REPEATS = 5\n",
    "results = {model: {\"Precision\": [], \"Recall\": [], \"AUC\": [], \"FTU\": [], \"DP\": []} for model in models}\n",
    "\n",
    "for seed in range(N_REPEATS):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "        # Save metrics\n",
    "        results[name][\"Precision\"].append(precision_score(y_test, y_pred))\n",
    "        results[name][\"Recall\"].append(recall_score(y_test, y_pred))\n",
    "        results[name][\"AUC\"].append(roc_auc_score(y_test, y_prob) if y_prob is not None else np.nan)\n",
    "\n",
    "        # === FTU ===\n",
    "        X_test_flipped = X_test.copy()\n",
    "        if sensitive_col in X_test_flipped.columns:\n",
    "            X_test_flipped[sensitive_col] = 1 - X_test_flipped[sensitive_col]\n",
    "            y_pred_flipped = model.predict(X_test_flipped)\n",
    "            ftu = np.mean(np.abs(y_pred - y_pred_flipped))\n",
    "        else:\n",
    "            ftu = np.nan\n",
    "        results[name][\"FTU\"].append(ftu)\n",
    "\n",
    "        # === DP (Demographic Parity) ===\n",
    "        if sensitive_col in X_test.columns:\n",
    "            mask_priv = X_test[sensitive_col] == 7\n",
    "            mask_unpriv = ~mask_priv\n",
    "            p_priv = y_pred[mask_priv].mean() if np.any(mask_priv) else 0\n",
    "            p_unpriv = y_pred[mask_unpriv].mean() if np.any(mask_unpriv) else 0\n",
    "            dp = abs(p_priv - p_unpriv)\n",
    "        else:\n",
    "            dp = np.nan\n",
    "        results[name][\"DP\"].append(dp)\n",
    "\n",
    "# === Format Results Table ===\n",
    "def format_metric(values):\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    return f\"{mean:.3f} ± {std:.3f}\"\n",
    "\n",
    "# Print Header\n",
    "print(f\"{'Model':<20} {'Precision↑':<15} {'Recall↑':<15} {'AUROC↑':<15} {'FTU↓':<15} {'DP↓':<15}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "# Print formatted table\n",
    "for model in models:\n",
    "    row = [model]\n",
    "    for metric in [\"Precision\", \"Recall\", \"AUC\", \"FTU\", \"DP\"]:\n",
    "        row.append(format_metric(results[model][metric]))\n",
    "    print(f\"{row[0]:<20} {row[1]:<15} {row[2]:<15} {row[3]:<15} {row[4]:<15} {row[5]:<15}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8376a8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                Precision↑      Recall↑         AUROC↑          FTU↓            DP↓            \n",
      "-----------------------------------------------------------------------------------------------\n",
      "Decision Tree        0.969 ± 0.006   0.975 ± 0.004   0.513 ± 0.030   0.013 ± 0.013   0.040 ± 0.044  \n",
      "Logistic Regression  0.968 ± 0.007   1.000 ± 0.000   0.453 ± 0.142   0.000 ± 0.000   0.000 ± 0.000  \n",
      "Random Forest        0.968 ± 0.007   1.000 ± 0.000   0.454 ± 0.172   0.000 ± 0.000   0.000 ± 0.000  \n",
      "SVM                  0.968 ± 0.007   1.000 ± 0.000   0.404 ± 0.185   0.000 ± 0.000   0.000 ± 0.000  \n",
      "XGBoost              0.968 ± 0.008   0.997 ± 0.003   0.479 ± 0.067   0.001 ± 0.002   0.018 ± 0.015  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# === Load dataset ===\n",
    "df = pd.read_csv(\"synthetic_law_data_decaf.csv\")    # DECAF Generated Data  for Law data\n",
    "df[\"bar_passed\"] = df[\"bar_passed\"].astype(int)\n",
    "sensitive_col = 'race'  # Update this if the sensitive feature is different\n",
    "\n",
    "# === Prepare features and labels ===\n",
    "X = df.drop(columns=[\"bar_passed\"])\n",
    "y = df[\"bar_passed\"]\n",
    "\n",
    "# === Initialize models ===\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
    "}\n",
    "\n",
    "# === Repeated Evaluation (for mean ± std) ===\n",
    "N_REPEATS = 5\n",
    "results = {model: {\"Precision\": [], \"Recall\": [], \"AUC\": [], \"FTU\": [], \"DP\": []} for model in models}\n",
    "\n",
    "for seed in range(N_REPEATS):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "        # Save metrics\n",
    "        results[name][\"Precision\"].append(precision_score(y_test, y_pred))\n",
    "        results[name][\"Recall\"].append(recall_score(y_test, y_pred))\n",
    "        results[name][\"AUC\"].append(roc_auc_score(y_test, y_prob) if y_prob is not None else np.nan)\n",
    "\n",
    "        # === FTU ===\n",
    "        X_test_flipped = X_test.copy()\n",
    "        if sensitive_col in X_test_flipped.columns:\n",
    "            X_test_flipped[sensitive_col] = 1 - X_test_flipped[sensitive_col]\n",
    "            y_pred_flipped = model.predict(X_test_flipped)\n",
    "            ftu = np.mean(np.abs(y_pred - y_pred_flipped))\n",
    "        else:\n",
    "            ftu = np.nan\n",
    "        results[name][\"FTU\"].append(ftu)\n",
    "\n",
    "        # === DP (Demographic Parity) ===\n",
    "        if sensitive_col in X_test.columns:\n",
    "            mask_priv = X_test[sensitive_col] == 7\n",
    "            mask_unpriv = ~mask_priv\n",
    "            p_priv = y_pred[mask_priv].mean() if np.any(mask_priv) else 0\n",
    "            p_unpriv = y_pred[mask_unpriv].mean() if np.any(mask_unpriv) else 0\n",
    "            dp = abs(p_priv - p_unpriv)\n",
    "        else:\n",
    "            dp = np.nan\n",
    "        results[name][\"DP\"].append(dp)\n",
    "\n",
    "# === Format Results Table ===\n",
    "def format_metric(values):\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    return f\"{mean:.3f} ± {std:.3f}\"\n",
    "\n",
    "# Print Header\n",
    "print(f\"{'Model':<20} {'Precision↑':<15} {'Recall↑':<15} {'AUROC↑':<15} {'FTU↓':<15} {'DP↓':<15}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "# Print formatted table\n",
    "for model in models:\n",
    "    row = [model]\n",
    "    for metric in [\"Precision\", \"Recall\", \"AUC\", \"FTU\", \"DP\"]:\n",
    "        row.append(format_metric(results[model][metric]))\n",
    "    print(f\"{row[0]:<20} {row[1]:<15} {row[2]:<15} {row[3]:<15} {row[4]:<15} {row[5]:<15}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96ba8ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model                Precision↑      Recall↑         AUROC↑          FTU↓            DP↓            \n",
      "-----------------------------------------------------------------------------------------------\n",
      "Decision Tree        0.912 ± 0.041   0.909 ± 0.018   0.902 ± 0.022   0.040 ± 0.015   0.115 ± 0.107  \n",
      "Logistic Regression  0.926 ± 0.033   0.927 ± 0.011   0.983 ± 0.004   0.013 ± 0.012   0.105 ± 0.117  \n",
      "Random Forest        0.926 ± 0.030   0.921 ± 0.032   0.981 ± 0.003   0.025 ± 0.013   0.111 ± 0.095  \n",
      "SVM                  0.866 ± 0.050   0.907 ± 0.014   0.951 ± 0.011   0.006 ± 0.006   0.074 ± 0.069  \n",
      "XGBoost              0.918 ± 0.048   0.929 ± 0.025   0.980 ± 0.004   0.039 ± 0.006   0.139 ± 0.079  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "# === Load dataset ===\n",
    "df = pd.read_csv(\"generated_data_CLLM_prompt_Law.csv\")    # CLLM Generated Data  for Law data\n",
    "df[\"bar_passed\"] = df[\"bar_passed\"].astype(int)\n",
    "sensitive_col = 'race'  # Update this if the sensitive feature is different\n",
    "\n",
    "# === Prepare features and labels ===\n",
    "X = df.drop(columns=[\"bar_passed\"])\n",
    "y = df[\"bar_passed\"]\n",
    "\n",
    "# === Initialize models ===\n",
    "models = {\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    \"SVM\": SVC(probability=True),\n",
    "    \"XGBoost\": XGBClassifier(eval_metric=\"logloss\", random_state=42)\n",
    "}\n",
    "\n",
    "# === Repeated Evaluation (for mean ± std) ===\n",
    "N_REPEATS = 5\n",
    "results = {model: {\"Precision\": [], \"Recall\": [], \"AUC\": [], \"FTU\": [], \"DP\": []} for model in models}\n",
    "\n",
    "for seed in range(N_REPEATS):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "        # Save metrics\n",
    "        results[name][\"Precision\"].append(precision_score(y_test, y_pred))\n",
    "        results[name][\"Recall\"].append(recall_score(y_test, y_pred))\n",
    "        results[name][\"AUC\"].append(roc_auc_score(y_test, y_prob) if y_prob is not None else np.nan)\n",
    "\n",
    "        # === FTU ===\n",
    "        X_test_flipped = X_test.copy()\n",
    "        if sensitive_col in X_test_flipped.columns:\n",
    "            X_test_flipped[sensitive_col] = 1 - X_test_flipped[sensitive_col]\n",
    "            y_pred_flipped = model.predict(X_test_flipped)\n",
    "            ftu = np.mean(np.abs(y_pred - y_pred_flipped))\n",
    "        else:\n",
    "            ftu = np.nan\n",
    "        results[name][\"FTU\"].append(ftu)\n",
    "\n",
    "        # === DP (Demographic Parity) ===\n",
    "        if sensitive_col in X_test.columns:\n",
    "            mask_priv = X_test[sensitive_col] == 7\n",
    "            mask_unpriv = ~mask_priv\n",
    "            p_priv = y_pred[mask_priv].mean() if np.any(mask_priv) else 0\n",
    "            p_unpriv = y_pred[mask_unpriv].mean() if np.any(mask_unpriv) else 0\n",
    "            dp = abs(p_priv - p_unpriv)\n",
    "        else:\n",
    "            dp = np.nan\n",
    "        results[name][\"DP\"].append(dp)\n",
    "\n",
    "# === Format Results Table ===\n",
    "def format_metric(values):\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    return f\"{mean:.3f} ± {std:.3f}\"\n",
    "\n",
    "# Print Header\n",
    "print(f\"{'Model':<20} {'Precision↑':<15} {'Recall↑':<15} {'AUROC↑':<15} {'FTU↓':<15} {'DP↓':<15}\")\n",
    "print(\"-\" * 95)\n",
    "\n",
    "# Print formatted table\n",
    "for model in models:\n",
    "    row = [model]\n",
    "    for metric in [\"Precision\", \"Recall\", \"AUC\", \"FTU\", \"DP\"]:\n",
    "        row.append(format_metric(results[model][metric]))\n",
    "    print(f\"{row[0]:<20} {row[1]:<15} {row[2]:<15} {row[3]:<15} {row[4]:<15} {row[5]:<15}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
